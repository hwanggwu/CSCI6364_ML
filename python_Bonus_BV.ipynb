{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "python_Bonus_BV.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hwanggwu/CSCI6364_ML/blob/master/python_Bonus_BV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HOpXuW6XBIp",
        "colab_type": "text"
      },
      "source": [
        "<body>\n",
        "<h2>Project 3: Bias Variance Trade-Off</h2>\n",
        "\n",
        "<!--announcements-->\n",
        "<blockquote>\n",
        "    <center>\n",
        "    <a href=\"http://blogs.worldbank.org/publicsphere/files/publicsphere/biased_processing.jpg\"><img src=\"bias.jpg\" width=\"600px\" /></a>\n",
        "    </center>\n",
        "      <p><cite><center>\"All of us show bias when it comes to what information we take in.<br>We typically focus on anything that agrees with the outcome we want.\"<br>\n",
        "<b>--Noreena Hertz</b>\n",
        "      </center></cite></p>\n",
        "</blockquote>\n",
        "<h3>Introduction</h3>\n",
        "\n",
        "<p>\n",
        "Recall that the squared error can be decomposed into <em>bias</em>, <em>variance</em> and <em>noise</em>: \n",
        "$$\n",
        "    \\underbrace{\\mathbb{E}[(h_D(x) - y)^2]}_\\mathrm{Error} = \\underbrace{\\mathbb{E}[(h_D(x)-\\bar{h}(x))^2]}_\\mathrm{Variance} + \\underbrace{\\mathbb{E}[(\\bar{h}(x)-\\bar{y}(x))^2]}_\\mathrm{Bias} + \\underbrace{\\mathbb{E}[(\\bar{y}(x)-y(x))^2]}_\\mathrm{Noise}\\nonumber\n",
        "$$\n",
        "We will now create a data set for which we can approximately compute this decomposition. \n",
        "The function <em><strong>`toydata`</strong></em> generates a binary data set with class $1$ and $2$. Both are sampled from Gaussian distributions:\n",
        "$$\n",
        "p(\\vec x|y=1)\\sim {\\mathcal{N}}(0,{I}) \\textrm { and } p(\\vec x|y=2)\\sim {\\mathcal{N}}(\\mu_2,{I}),\n",
        "$$\n",
        "\n",
        "where $\\mu_2=[2;2]^\\top$ (the global variable <em>OFFSET</em> $\\!=\\!2$ regulates these values: $\\mu_2=[$<em>OFFSET</em> $;$ <em>OFFSET</em>$]^\\top$).\n",
        "</p>\n",
        "\n",
        "<h3>Computing noise, bias and variance</h3>\n",
        "<p>\n",
        "You will need to edit three functions:  <em><strong>`computeybar`</strong></em>,  <em><strong>`computehbar`</strong></em>, and <em><strong>`computevariance`</strong></em>. First take a look at <strong>`biasvariancedemo`</strong> and make sure you understand where each function should be called and how they contribute to the Bias/Variance/Noise decomposition. <br/><br/>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Er3d3mhrXBIq",
        "colab_type": "text"
      },
      "source": [
        "**Libraries**: Before we get started we need to install a few libraries. You can do this by executing the following code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7ItZoyzXBIr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#<GRADED>\n",
        "import numpy as np\n",
        "from numpy.matlib import repmat\n",
        "#</GRADED>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "eNVky71rXBIv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib\n",
        "#matplotlib.use('PDF')\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "from scipy.io import loadmat\n",
        "import time\n",
        "\n",
        "%matplotlib notebook"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcZmazivXBIy",
        "colab_type": "text"
      },
      "source": [
        "**`l2distance` Helper Function**: `l2distance` is a helper function used in our implementation of the ridge regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfrtXDMCXBIy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#<GRADED>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Atg60d-XBI1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def l2distance(X, Z=None):\n",
        "    \"\"\"\n",
        "    function D=l2distance(X,Z)\n",
        "\n",
        "    Computes the Euclidean distance matrix.\n",
        "    Syntax:\n",
        "    D=l2distance(X,Z)\n",
        "    Input:\n",
        "    X: dxn data matrix with n vectors (columns) of dimensionality d\n",
        "    Z: dxm data matrix with m vectors (columns) of dimensionality d\n",
        "\n",
        "    Output:\n",
        "    Matrix D of size nxm\n",
        "    D(i,j) is the Euclidean distance of X(:,i) and Z(:,j)\n",
        "\n",
        "    call with only one input:\n",
        "    l2distance(X)=l2distance(X,X)\n",
        "    \"\"\"\n",
        "    if Z is None:\n",
        "        n, d = X.shape\n",
        "        s1 = np.sum(np.power(X, 2), axis=1).reshape(-1,1)\n",
        "        D1 = -2 * np.dot(X, X.T) + repmat(s1, 1, n)\n",
        "        D = D1 + repmat(s1.T, n, 1)\n",
        "        np.fill_diagonal(D, 0)\n",
        "        D = np.sqrt(np.maximum(D, 0))\n",
        "    else:\n",
        "        n, d = X.shape\n",
        "        m, _ = Z.shape\n",
        "        s1 = np.sum(np.power(X, 2), axis=1).reshape(-1,1)\n",
        "        s2 = np.sum(np.power(Z, 2), axis=1).reshape(1,-1)\n",
        "        D1 = -2 * np.dot(X, Z.T) + repmat(s1, 1, m)\n",
        "        D = D1 + repmat(s2, n, 1)\n",
        "        D = np.sqrt(np.maximum(D, 0))\n",
        "    return D\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mHaLKZfXBI4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#</GRADED>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWj21NwAXBI7",
        "colab_type": "text"
      },
      "source": [
        "**`toydata` Helper Function**: `toydata` is a helper function used to generate the the binary data with n/2 values in class 1 and n/2 values in class 2. With class 1 being the label for data drawn from a normal distribution having mean 0 and sigma 1. And clss 2 being the label for data drawn from a normal distribution with mean OFFSET and sigma 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ibDzWw4XBI8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#<GRADED>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nl8fo_D0XBI_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def toydata(OFFSET,N):\n",
        "    \"\"\"\n",
        "    function [x,y]=toydata(OFFSET,N)\n",
        "    \n",
        "    This function constructs a binary data set. \n",
        "    Each class is distributed by a standard Gaussian distribution.\n",
        "    INPUT: \n",
        "    OFFSET:  Class 1 has mean 0,  Class 2 has mean 0+OFFSET (in each dimension). \n",
        "    N: The function returns N data points ceil(N/2) are of class 2, the rest\n",
        "    of class 1\n",
        "    \"\"\"\n",
        "    \n",
        "    NHALF = int(np.ceil(N/2))\n",
        "    x = np.random.randn(N, 2)\n",
        "    x[NHALF:, :] += OFFSET  \n",
        "    \n",
        "    y = np.ones(N)\n",
        "    y[NHALF:] *= 2\n",
        "    \n",
        "    jj = np.random.permutation(N)\n",
        "    return x[jj, :], y[jj]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9cJPJEcXBJC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#</GRADED>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYYEZ91nXBJH",
        "colab_type": "text"
      },
      "source": [
        "<p>\n",
        "(a) <strong>Noise:</strong> First we focus on the noise. For this, you need to compute $\\bar y(\\vec x)$ in  <em><strong>`computeybar`</strong></em>. You can compute the probability $p(\\vec x|y)$ with the equations $p(\\vec x|y=1)\\sim {\\mathcal{N}}(0,{I}) \\textrm { and } p(\\vec x|y=2)\\sim {\\mathcal{N}}(\\mu_2,{I})$. Then use Bayes rule to compute $p(y|\\vec x)$. <br/><br/>\n",
        "<strong>Note:</strong> You may want to use the function <em>`normpdf`</em>, which is defined for  you in <em><strong>`computeybar`</strong></em>.\n",
        "<br/><br/></p>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAL4ypM4XBJH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#<GRADED>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IesTUFiAXBJK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def computeybar(xTe, OFFSET):\n",
        "    \"\"\"\n",
        "    function [ybar]=computeybar(xTe, OFFSET);\n",
        "\n",
        "    computes the expected label 'ybar' for a set of inputs x\n",
        "    generated from two standard Normal distributions (one offset by OFFSET in\n",
        "    both dimensions.)\n",
        "\n",
        "    INPUT:\n",
        "    xTe : nx2 array of n vectors with 2 dimensions\n",
        "    OFFSET    : The OFFSET passed into the toyData function. The difference in the\n",
        "                mu of labels class1 and class2 for toyData.\n",
        "\n",
        "    OUTPUT:\n",
        "    ybar : a nx1 vector of the expected labels for vectors xTe\n",
        "    \"\"\"\n",
        "    n,temp = xTe.shape\n",
        "    ybar = np.zeros(n)\n",
        "\n",
        "    normpdf = lambda x, mu, sigma: np.exp(-0.5 * np.power((x - mu) / sigma, 2)) / (np.sqrt(2 * np.pi) * sigma)\n",
        "    \n",
        "    px1 = np.zeros([1,n]);\n",
        "    px2 = np.zeros([1,n]);\n",
        "    sigma = np.ones([2,n]);\n",
        "\n",
        "    for i in range(1,n): \n",
        "      px1[1,i] = normpdf(xTe[1:,i],0,sigma)\n",
        "\n",
        "    for j in range (1,n):\n",
        "      px2[1,j] = normpdf(xTe[1:,i],OFFSET,sigma) \n",
        "\n",
        "\n",
        "    py1 = px1/(px1+px2);\n",
        "    py2 = px2/(px1+px2);\n",
        "    ybar = 1*py1 + 2*py2;\n",
        "\n",
        "    raise NotImplementedError('Your code goes here!')\n",
        "    \n",
        "    return ybar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRTMfjJDdVYC",
        "colab_type": "text"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwI6unCNXBJN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#</GRADED>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV062nBgXBJP",
        "colab_type": "text"
      },
      "source": [
        "**Visualizing the Data**:\n",
        "You can now see the error of the bayes classifier. Below is a plotting of the two classes of points and the misclassified points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6W9bhZwXBJQ",
        "colab_type": "code",
        "outputId": "45b90e90-3c2f-4d0f-a8e4-c61f386efb23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        }
      },
      "source": [
        "OFFSET = 2\n",
        "xTe, yTe = toydata(OFFSET, 1000)\n",
        "\n",
        "# compute Bayes Error\n",
        "ybar = computeybar(xTe, OFFSET)\n",
        "predictions = np.round(ybar)\n",
        "errors = predictions != yTe\n",
        "err = errors.sum() / len(yTe) * 100\n",
        "print('Error of Bayes classifier: %.1f%%.' % err)\n",
        "\n",
        "# plot data\n",
        "i1 = yTe == 1\n",
        "i2 = yTe == 2\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.scatter(xTe[i1, 0], xTe[i1, 1], c='r', marker='o')\n",
        "plt.scatter(xTe[i2, 0], xTe[i2, 1], c='b', marker='o')\n",
        "plt.scatter(xTe[errors, 0], xTe[errors, 1], c='k', s=100, alpha=0.2)\n",
        "plt.title(\"Plot of data (misclassified points highlighted)\")\n",
        "plt.show()"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-90-37f9f84a09d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# compute Bayes Error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mybar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomputeybar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxTe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOFFSET\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mybar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0myTe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-89-3a7a86aa1543>\u001b[0m in \u001b[0;36mcomputeybar\u001b[0;34m(xTe, OFFSET)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m       \u001b[0mpx1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormpdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxTe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-89-3a7a86aa1543>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x, mu, sigma)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mybar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mnormpdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mpx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (999,) (2,1000) "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyZmHSuZXBJT",
        "colab_type": "text"
      },
      "source": [
        "<p>With the help of <strong>`computeybar`</strong> you can now compute the \"noise\" variable within <strong>`biasvariancedemo`</strong>. </p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0Nf_uT8XBJT",
        "colab_type": "text"
      },
      "source": [
        "**`kregression` Helper Function**: \n",
        "<br/>\n",
        "<strong>Important</strong> - $h_D$ is defined for you in <em><strong>`kregression`</strong></em>. It's kernelized ridge regression with kernel width $\\sigma$ and regularization constant $\\lambda$.\n",
        "<br/><br/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9ANN-xwXBJU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#<GRADED>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmBEvMBsXBJX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def kregression(xTr,yTr,sigma=0.1,lmbda=0.01):\n",
        "    \"\"\"\n",
        "    function kregression(xTr,yTr,sigma,lmbda)\n",
        "    \n",
        "    Input:\n",
        "    xTr | training data (nx2)\n",
        "    yTr | training labels (nx1)\n",
        "    sigma | kernel width (>0)\n",
        "    lmbda | regularization constant (>0)\n",
        "    \n",
        "    Output:\n",
        "    fun | usage: predictions=fun(xTe);\n",
        "    \"\"\"\n",
        "    kernel = lambda x, z: np.power(1+(np.power(l2distance(x,z),2) / (2 * np.power(sigma,2))),-4)\n",
        "    ridge = lambda K, lmbda2: K + lmbda * np.eye(K.shape[0], K.shape[1])\n",
        "    beta = np.linalg.solve(ridge(kernel(xTr, xTr), lmbda), yTr)\n",
        "    \n",
        "    fun = lambda Xt: np.dot(kernel(Xt, xTr), beta)\n",
        "    return fun"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbVBHOldXBJb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#</GRADED>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GitmhO8LXBJd",
        "colab_type": "text"
      },
      "source": [
        "<p>\n",
        "(b) <strong>Bias:</strong> For the bias, you will need $\\bar{h}$. Although we cannot compute the expected value  $\\bar h\\!=\\!\\mathbb{E}[h]$, we can approximate it by training many $h_D$ and averaging their predictions. Edit the file <em><strong>`computehbar`</strong></em>. Average over <em>NMODELS</em> different $h_D$, each trained on a different data set of <em>Nsmall</em> inputs drawn from the same distribution. Feel free to call <em><strong>`toydata`</strong></em> to obtain more data sets. <br/><br/>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbD_jV2XXBJe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#<GRADED>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOymmQrsXBJh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def computehbar(xTe, sigma, lmbda, Nsmall, NMODELS, OFFSET):\n",
        "    \"\"\"\n",
        "    function [hbar]=computehbar(xTe, sigma, lmbda, NSmall, NMODELS, OFFSET);\n",
        "\n",
        "    computes the expected prediction of the average classifier (hbar)\n",
        "    for data set xTe. \n",
        "\n",
        "    The training data of size Nsmall is drawn from toydata with OFFSET \n",
        "    with kernel regression with sigma and lmbda\n",
        "\n",
        "    The \"infinite\" number of models is estimated as an average over NMODELS. \n",
        "\n",
        "    INPUT:\n",
        "    xTe       | nx2 matrix, of n column-wise input vectors (each 2-dimensional)\n",
        "    sigma     | kernel width of the RBF kernel\n",
        "    lmbda     | regularization constant\n",
        "    NSmall    | Number of points to subsample\n",
        "    NMODELS   | Number of Models to average over\n",
        "    OFFSET    | The OFFSET passed into the toyData function. The difference in the\n",
        "                mu of labels class1 and class2 for toyData.\n",
        "    OUTPUT:\n",
        "    hbar | nx1 vector with the predictions of hbar for each test input\n",
        "    \"\"\"\n",
        "    n = xTe.shape[0]\n",
        "    hbar = np.zeros(n)\n",
        "    for j in range(NMODELS):\n",
        "        ## fill in code here\n",
        "        fun = kregression(xTr, yTr, sigma, lmbda);\n",
        "        hd = fun(xTe);\n",
        "        hbar = hbar + hd;\n",
        "        raise NotImplementedError('Your code goes here!')\n",
        "    hbar /= NMODELS\n",
        "    return hbar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHNa1_wnXBJj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#</GRADED>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlVnbQKxXBJl",
        "colab_type": "text"
      },
      "source": [
        "<p>With the help of <strong>`computehbar`</strong> you can now compute the \"bias\" variable within <strong>`biasvariancedemo`</strong>. </p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XK3L5TmAXBJm",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "<p>(c) <strong>Variance:</strong> Finally, to compute the variance, we need to compute the term $\\mathbb{E}[(h_D-\\bar{h})^2]$. Once again, we can approximate this term by averaging over  <em>NMODELS</em> models. Edit the file <em><strong>`computevariance`</strong></em>. \n",
        "<br/></br></p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DyXzwk-XBJm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#<GRADED>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9Ei8kisXBJo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def computevariance(xTe, sigma, lmbda, hbar, Nsmall, NMODELS, OFFSET):\n",
        "    \"\"\"\n",
        "    function variance=computevariance(xTe,sigma,lmbda,hbar,Nsmall,NMODELS,OFFSET)\n",
        "\n",
        "    computes the variance of classifiers trained on data sets from\n",
        "    toydata.m with pre-specified \"OFFSET\" and \n",
        "    with kernel regression with sigma and lmbda\n",
        "    evaluated on xTe. \n",
        "    the prediction of the average classifier is assumed to be stored in \"hbar\".\n",
        "\n",
        "    The \"infinite\" number of models is estimated as an average over NMODELS. \n",
        "\n",
        "    INPUT:\n",
        "    xTe       : nx2 matrix, of n column-wise input vectors (each 2-dimensional)\n",
        "    sigma     : kernel width of the RBF kernel\n",
        "    lmbda     : regularization constant\n",
        "    hbar      : nx1 vector of the predictions of hbar on the inputs xTe\n",
        "    Nsmall    : Number of samples drawn from toyData for one model\n",
        "    NModel    : Number of Models to average over\n",
        "    OFFSET    : The OFFSET passed into the toyData function. The difference in the\n",
        "                mu of labels class1 and class2 for toyData.\n",
        "    \"\"\"\n",
        "    n = xTe.shape[0]\n",
        "    variance = np.zeros(n)\n",
        "    \n",
        "    for j in range(NMODELS):\n",
        "        (xTr,yTr)= toydata(OFFSET,Nsmall);\n",
        "        fun = kregression(xTr, yTr, sigma, lmbda);\n",
        "        hd = fun(xTe);\n",
        "        variance = variance + numpy.square(hd-hbar);     \n",
        "        raise NotImplementedError('Your code goes here!')\n",
        "    \n",
        "    variance = np.mean(variance)/NMODELS\n",
        "    return variance"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxoYMdKtXBJq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#</GRADED>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "op6LtK1UXBJs",
        "colab_type": "text"
      },
      "source": [
        "<p>With the help of <strong>`computevariance`</strong> you can now compute the \"variance\" variable within <strong>`biasvariancedemo`</strong>. </p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54b4YHAbXBJt",
        "colab_type": "text"
      },
      "source": [
        "<p>If you did everything correctly and call execute the following demo. You should see how the error decomposes (roughly) into bias, variance and noise when regularization constant $\\lambda$ increases.</p>\n",
        "<br/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUrpf808XBJu",
        "colab_type": "code",
        "outputId": "1c7666dc-5e02-434c-8015-a3490a217459",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "# biasvariancedemo\n",
        "\n",
        "# how big is the training set size N\n",
        "Nsmall = 10\n",
        "# how big is a really big data set (approx. infinity)\n",
        "Nbig = 10000\n",
        "# how many models do you want to average over\n",
        "NMODELS = 100\n",
        "# What regularization constants to evaluate\n",
        "lmbdas = np.arange(-6, 0+0.5, 0.5)\n",
        "# what is the kernel width?\n",
        "sigma = 4\n",
        "\n",
        "# we store\n",
        "Nlambdas = len(lmbdas)\n",
        "lbias = np.zeros(Nlambdas)\n",
        "lvariance = np.zeros(Nlambdas)\n",
        "ltotal = np.zeros(Nlambdas)\n",
        "lnoise = np.zeros(Nlambdas)\n",
        "lsum = np.zeros(Nlambdas)\n",
        "\n",
        "# Different regularization constant classifiers\n",
        "for md in range(Nlambdas):\n",
        "    lmbda = 2 ** lmbdas[md]\n",
        "    # use this data set as an approximation of the true test set\n",
        "    xTe,yTe = toydata(OFFSET,Nbig)\n",
        "    \n",
        "    # Estimate AVERAGE ERROR (TOTAL)\n",
        "    total = 0\n",
        "    for j in range(NMODELS):\n",
        "        xTr2,yTr2 = toydata(OFFSET,Nsmall)\n",
        "        fsmall = kregression(xTr2,yTr2,sigma,lmbda)\n",
        "        total += np.mean((fsmall(xTe) - yTe) ** 2)\n",
        "    total /= NMODELS\n",
        "    \n",
        "    # Estimate Noise\n",
        "    ybar = computeybar(xTe, OFFSET)\n",
        "    noise = np.mean((yTe-ybar) ** 2)\n",
        "    \n",
        "    # Estimate Bias\n",
        "    hbar = computehbar(xTe,sigma, lmbda, Nsmall, NMODELS, OFFSET)\n",
        "    bias = np.mean((hbar-ybar) ** 2)\n",
        "    \n",
        "    # Estimating VARIANCE\n",
        "    variance = computevariance(xTe,sigma,lmbda,hbar, Nsmall, NMODELS, OFFSET)\n",
        "    \n",
        "    # print and store results\n",
        "    lbias[md] = bias\n",
        "    lvariance[md] = variance\n",
        "    ltotal[md] = total\n",
        "    lnoise[md] = noise\n",
        "    lsum[md] = lbias[md]+lvariance[md]+lnoise[md]\n",
        "    print('Regularization Î»=2^%2.1f: Bias: %2.4f Variance: %2.4f Noise: %2.4f Bias+Variance+Noise: %2.4f Test error: %2.4f'\n",
        "          % (lmbdas[md],lbias[md],lvariance[md],lnoise[md],lsum[md],ltotal[md]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-75-3db060502097>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# Estimate Noise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mybar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomputeybar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxTe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOFFSET\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myTe\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mybar\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# Estimate Bias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (10000,) (10000,2) "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwGnv3FEXBJy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot results\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(lbias[:Nlambdas],c='r',linestyle='-',linewidth=2)\n",
        "plt.plot(lvariance[:Nlambdas],c='k', linestyle='-',linewidth=2)\n",
        "plt.plot(lnoise[:Nlambdas],c='g', linestyle='-',linewidth=2)\n",
        "plt.plot(ltotal[:Nlambdas],c='b', linestyle='-',linewidth=2)\n",
        "plt.plot(lsum[:Nlambdas],c='k', linestyle='--',linewidth=2)\n",
        "\n",
        "plt.legend([\"Bias\",\"Variance\",\"Noise\",\"Test error\",\"Bias+Var+Noise\"]);\n",
        "plt.xlabel(\"Regularization $\\lambda=2^x$\",fontsize=18);\n",
        "plt.ylabel(\"Squared Error\",fontsize=18);\n",
        "plt.xticks([i for i in range(Nlambdas)],lmbdas);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLZ_k3dGXBJ0",
        "colab_type": "text"
      },
      "source": [
        "Feel free to modify $\\lambda$/$\\sigma$ in these two files. If you want the approximation to be more accurate, increase <em>NMODELS</em> and/or <em>Nbig</em> (the more models you train, the better your approximation will be for $\\mathbb{E}[h]$ and $\\mathbb{E}[(h_D-\\bar{h})^2]$). \n",
        "You can also play around with the variable <em>Nsmall</em> which regulates how big your actual training is supposed to be. \n",
        "</p>\n",
        "\n",
        "\n",
        "<h3>Note</h3>\n",
        "<p>\n",
        "When computing the bias and variance, you approximate the results by training many $h_D$. We set <em>NMODELS</em>=1000 and use some thresholds to test if your functions' results are correct. Unfortunately, as a result of this randomness, there is still a small chance that you will fail some test cases, even though your implementations are correct. <br/><br/>\n",
        "If you can pass all the tests most of the times locally, then you are fine. In this case, if the autograder says your accuracy is not 100%, just commit the code again.<br/><br/>\n",
        "\n",
        "There is no competition this time.\n",
        "</p>"
      ]
    }
  ]
}